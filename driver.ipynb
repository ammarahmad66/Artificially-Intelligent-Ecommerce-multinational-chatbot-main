{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G38dGewGHMqx",
        "outputId": "e0751e07-a719-4049-b1f7-e2e57a0febe4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import random\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJHuxfl2HN__"
      },
      "outputs": [],
      "source": [
        "stemmer=PorterStemmer()\n",
        "\n",
        "def tokenize(query):\n",
        "    return nltk.word_tokenize(query)\n",
        "\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "def bags(tokenized,word_list):\n",
        "    tokens=[stem(w) for w in tokenized]\n",
        "    bag=np.zeros(len(word_list), dtype=np.float32)\n",
        "\n",
        "    for i in range(len(word_list)):\n",
        "        if word_list[i] in tokens:\n",
        "            bag[i]=1.0\n",
        "\n",
        "    return bag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymawqAs3HjIp",
        "outputId": "4c8225f0-99c1-494e-8f3b-54ec3726feae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch100/1000, loss=1.1042413711547852\n",
            "epoch200/1000, loss=0.5161978602409363\n",
            "epoch300/1000, loss=0.5220367312431335\n",
            "epoch400/1000, loss=0.013459883630275726\n",
            "epoch500/1000, loss=0.003509614383801818\n",
            "epoch600/1000, loss=0.9743396043777466\n",
            "epoch700/1000, loss=0.0009140105685219169\n",
            "epoch800/1000, loss=0.4873718321323395\n",
            "epoch900/1000, loss=0.48675110936164856\n",
            "epoch1000/1000, loss=0.973254919052124\n",
            "final loss=0.973254919052124\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Model(\n",
              "  (l1): Linear(in_features=54, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=7, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Preparing the Training Data\n",
        "\n",
        "desc = open('intents.json','r') #tags,patterns,responses\n",
        "data=json.load(desc)\n",
        "\n",
        "words_list=[]\n",
        "tags=[]\n",
        "training_data=[]\n",
        "\n",
        "for intent in data['intents']:\n",
        "    tag=intent['tag']\n",
        "    tags.append(tag)\n",
        "\n",
        "    for pattern in intent['patterns']:\n",
        "      w=tokenize(pattern)\n",
        "      words_list.extend(w) #Keeping Track of all the words for bag-of-words\n",
        "      training_data.append((w,tag)) #X-Y Training Data\n",
        "\n",
        "punctuations=['?','!','.','?']\n",
        "buff=[]\n",
        "\n",
        "trainx=[]\n",
        "trainy=[]\n",
        "\n",
        "buff=[stem(x) for x in words_list if x not in punctuations]\n",
        "\n",
        "words_list=sorted(set(buff))\n",
        "tags=sorted(set(tags))\n",
        "\n",
        "for (tokens,tag) in training_data:\n",
        "    trainx.append(bags(tokens,words_list)) #X -> Bag Of Words\n",
        "    trainy.append(tags.index(tag)) #Y -> Tags for Each Sentence\n",
        "\n",
        "trainx=np.array(trainx)\n",
        "trainy=np.array(trainy)\n",
        "\n",
        "\n",
        "#Create Dataset Loader\n",
        "class CreateData(Dataset):\n",
        "  def __init__(self):\n",
        "    self.samples=len(trainx)\n",
        "    self.datax=trainx\n",
        "    self.datay=trainy\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.datax[index],self.datay[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.samples\n",
        "\n",
        "\n",
        "dataset1=CreateData()\n",
        "loader=DataLoader(dataset=dataset1, batch_size=8,shuffle=True,num_workers=2)\n",
        "\n",
        "\n",
        "#Creating a Class For Neural Network Model\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,num_classes):\n",
        "    super(Model,self).__init__()\n",
        "    self.l1=nn.Linear(input_size,hidden_size)\n",
        "    self.l2=nn.Linear(hidden_size,hidden_size)\n",
        "    self.l3=nn.Linear(hidden_size,num_classes)\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out=self.l1(x)\n",
        "    out=self.relu(out)\n",
        "    out=self.l2(out)\n",
        "    out=self.relu(out)\n",
        "    out=self.l3(out)\n",
        "    out=self.relu(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "#Setting Epochs\n",
        "epochs=1000\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=Model(len(words_list),8,(len(tags)))\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(1000):\n",
        "  for(x,y) in loader:\n",
        "    words=x.to(device)\n",
        "    labels=y.to(device)\n",
        "\n",
        "    outputs=model(words)\n",
        "    lossy=loss(outputs,labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    lossy.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  if (epoch+1)%100==0:\n",
        "    print(f'epoch{epoch+1}/{epochs}, loss={lossy.item()}')\n",
        "\n",
        "print(f'final loss={lossy.item()}')\n",
        "model.eval()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hoz3a97Qc4NH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "pTdbS22oRWnI",
        "outputId": "935c7367-0eb2-4fb5-c958-1412bb31f59a"
      },
      "outputs": [],
      "source": [
        "#Main Driver\n",
        "\n",
        "print(\"How may we help you :) (type 'quit' to exit)\")\n",
        "learning=[]\n",
        "while True:\n",
        "  sentence = input(\"You: \")\n",
        "  if sentence == \"quit\":\n",
        "    answ=os.path.exists(\"learning.txt\")\n",
        "    with open(\"learning.txt\", \"a\" if answ else \"w\") as f:\n",
        "      f.writelines(learning)    \n",
        "      break\n",
        "\n",
        "  sent1 = tokenize(sentence)\n",
        "  input_val = bags(sent1, words_list)\n",
        "  input_val = input_val.reshape(1, input_val.shape[0])\n",
        "  input_val = torch.from_numpy(input_val).to(device)\n",
        "\n",
        "  output = model(input_val)\n",
        "  _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "  tag = tags[predicted.item()]\n",
        "\n",
        "  probs = torch.softmax(output, dim=1)\n",
        "  prob=probs[0][predicted.item()]\n",
        "   \n",
        "    \n",
        "  if prob.item() > 0.65:\n",
        "      for intent in data['intents']:\n",
        "          if tag == intent[\"tag\"]:\n",
        "              print(f\"Sam: {random.choice(intent['responses'])}\")\n",
        "  else:\n",
        "    learning.append(sentence+'\\n')\n",
        "    print(\"Sam: Sorry, I can't understand but your response is recorded and will be responded asap :)\")\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "zDcl0pvLpsQi",
        "outputId": "b7cc66fd-2481-4183-849b-04dda58ed652"
      },
      "outputs": [],
      "source": [
        "import json\n",
        " \n",
        " \n",
        "#Self Leaning Mechanism\n",
        "def write_json(new_data, filename='intents.json'):\n",
        "  check=False\n",
        "  with open(filename,'r+') as file:\n",
        "      file_data = json.load(file)\n",
        "      for intent in file_data['intents']:\n",
        "        if intent['tag']==new_data['tag']:\n",
        "          check=True\n",
        "          intent['patterns'].extend(new_data['patterns'])\n",
        "          intent['responses'].extend(new_data['responses'])\n",
        "        # Sets file's current position at offset.\n",
        "      \n",
        "      if check==False:\n",
        "        file_data['intents'].append(new_data)\n",
        "        \n",
        "\n",
        "      file.seek(0)\n",
        "        # convert back to json.\n",
        "      json.dump(file_data, file, indent = 4)\n",
        "\n",
        "\n",
        "c=\"\"\n",
        "dict1={}\n",
        "with open(\"learning.txt\",'r') as file:\n",
        "    c=file.readline()\n",
        "    while c:\n",
        "      print(\"Query: \",c)\n",
        "      tag=input(\"Enter its Tag:\")\n",
        "      response=input(\"Enter its response:\")\n",
        "      dict1[\"tag\"]=tag\n",
        "      dict1[\"patterns\"]=list([c[:-1]])\n",
        "      dict1[\"responses\"]=list([response])\n",
        "      write_json(dict1)\n",
        "      dict1.clear()\n",
        "      c=file.readline()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8JAK0LHS7l9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
